{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellosmallkat/Movie-Recommendation-System/blob/main/Copy_of_Movie_Recommendation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\">\n",
        "    NSDC Data Science Projects\n",
        "</h1>\n",
        "  \n",
        "<h2 align=\"center\">\n",
        "    Project: Movie Recommendation System\n",
        "</h2>\n",
        "\n",
        "<h3 align=\"center\">\n",
        "    Name: hellosmallkat\n",
        "</h3>\n"
      ],
      "metadata": {
        "id": "RTUk7S0GxzNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Please read before you begin your project**\n",
        "\n",
        "**Instructions: Google Colab Notebooks:**\n",
        "\n",
        "Google Colab is a free cloud service. It is a hosted Jupyter notebook service that requires no setup to use, while providing free access to computing resources. We will be using Google Colab for this project.\n",
        "\n",
        "Certain parts of this project will be completed individually, while other parts are encouraged to be completed with the rest of your team. In order to work within the Google Colab Notebook, **please start by clicking on \"File\" and then \"Save a copy in Drive.\"** This will save a copy of the notebook in your personal Google Drive. Each member of your team should work on their personal copy.\n",
        "\n",
        "Please rename the file to \"Movie Recommendation Analysis - Your Full Name.\" Once this project is completed, you will be prompted to share your file with the National Student Data Corps (NSDC) Project Leaders.\n",
        "\n",
        "You can now start working on the project. :)"
      ],
      "metadata": {
        "id": "duXjuu5Ax1-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using Google Colab for this assignment. This is a Python Notebook environment built by Google that's free for everyone and comes with a nice UI out of the box. For a comprehensive guide, see Colab's official guide [here](https://colab.research.google.com/github/prites18/NoteNote/blob/master/Welcome_To_Colaboratory.ipynb).\n",
        "\n",
        "Colab QuickStart:\n",
        "- Notebooks are made up of cells, cells can be either text or code cells. Click the +code or +text button at the top to create a new cell\n",
        "- Text cells use a format called [Markdown](https://www.markdownguide.org/getting-started/). Cheatsheet is available [here](https://www.markdownguide.org/cheat-sheet/)\n",
        "- Python code is run/executed in code cells. You can click the play button at the top left of a code block (sometimes hidden in the square brackets) to run the code in that cell. You an also hit shift+enter to run the cell that is currently selected. There is no concurrency since cells run one at a time but you can queue up multiple cells\n",
        "- Each cell will run code individually but memory is shared across a notebook Runtime. You can think of a Runtime as a code session where everything you create and execute is temporarily stored. This means variables and functions are available between cells if you execute one cell before the other (physical ordering of cells does not matter). This also means that if you delete or change the name of something and re-execute the cell, the old data might still exist in the background. If things aren't making sense, you can always click Runtime -> restart runtime to start over.\n",
        "- Runtimes will persist for a short period of time so you are safe if you lose connection or refresh the page but Google will shutdown a runtime after enough time has past. Everything that was printed out will remain on the page even if the runtime is disconnected\n",
        "- Google's Runtimes come preinstalled with all the core python libraries (math, rand, time, etc) as well as common data analysis libraries (numpy, pandas, scikitlearn, matplotlib). Simply run `import numpy as np` in a code cell to make it available"
      ],
      "metadata": {
        "id": "WHba4I24yBt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Singular Value Decomposition**\n",
        "\n",
        "---\n",
        "\n",
        "# Crash Course on Singular Value Decomposition (SVD)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Singular Value Decomposition, or SVD, is a mathematical technique used in many fields such as signal processing, statistics, and machine learning, particularly in the context of recommendation systems. It's a method for decomposing a matrix into three other matrices that reveal its underlying structure.\n",
        "\n",
        "## Basic Concepts\n",
        "\n",
        "### Matrices\n",
        "- **Matrix**: A rectangular array of numbers.\n",
        "- **Dimension of a Matrix**: Given in the form of rows × columns.\n",
        "\n",
        "### Decomposition\n",
        "- **Decomposition**: Breaking down a complex matrix into simpler, understandable parts.\n",
        "\n",
        "## What is SVD?\n",
        "\n",
        "```\n",
        "SVD breaks down any given matrix A into three separate matrices named U, Σ and V*\n",
        "ie. A = UΣV*\n",
        "```\n",
        "Where the components are:\n",
        "```\n",
        "- A: Original matrix.\n",
        "- U: Left singular vectors (orthogonal matrix).\n",
        "- Σ: Diagonal matrix of singular values (non-negative).\n",
        "- V*: Right singular vectors (conjugate transpose of V , an orthogonal matrix).\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "81bBnH8IyJIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where do we use SVDs?\n",
        "\n",
        "### Applications in Recommendation Systems\n",
        "\n",
        "In recommendation systems, SVD is used to predict unknown preferences by decomposing a large matrix of user-item interactions into factors representing latent features. It helps in capturing the underlying patterns in the data.\n",
        "\n",
        "### Process\n",
        "\n",
        "1. **Matrix Creation**: Start with a matrix where rows represent users, columns represent items, and entries represent user ratings.\n",
        "2. **Apply SVD**: Decompose this matrix using SVD.\n",
        "3. **Latent Features**: The decomposition reveals latent features that explain observed ratings.\n",
        "4. **Prediction**: Use the decomposed matrices to predict missing ratings.\n",
        "\n",
        "### Advantages of an SVD\n",
        "- Effective at uncovering latent features in the data.\n",
        "- Reduces dimensionality, making computations more manageable.\n",
        "\n",
        "### Limitations of an SVD\n",
        "- Assumes linear relationships in data.\n",
        "- Sensitive to missing data and outliers."
      ],
      "metadata": {
        "id": "dLwYe8rLJY_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Through this project, we will learn how to build a movie recommendation system using an SVD\n",
        "\n",
        "\n",
        "#### Dataset being used : **Movielens 100k dataset**\n",
        "\n",
        "- This specific dataset, often referred to as \"ml-100k,\" contains 100,000 ratings from 943 users on 1,682 movies. The data was collected through the MovieLens website during the seven-month period from September 19th, 1997 to April 22nd, 1998.\n",
        "\n",
        "- **Data Structure**: The dataset includes user ratings that range from 1 to 5. Additionally, it provides demographic information about the users (age, gender, occupation, etc.) and details about the movies (titles, genres).\n",
        "\n",
        "- **Usage**: It's a standard dataset used for implementing and testing recommender systems. Its size is manageable, making it a popular choice for educational purposes and for initial experimentation with recommendation algorithms.\n",
        "\n",
        "- **Significance**: The diversity in the dataset, both in terms of users and movie genres, provides a rich ground for analyzing different recommendation strategies, testing algorithms like SVD, and understanding user preferences and behavioral patterns.\n",
        "\n",
        "This dataset is an excellent starting point for anyone looking to delve into the world of recommender systems and practice with real-world data.\n",
        "\n",
        "\n",
        "Now, we will write some code to understand and explore the dataset"
      ],
      "metadata": {
        "id": "C_2UqcW9JmGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this only once, you can comment out this part of the code after.\n",
        "!pip install surprise"
      ],
      "metadata": {
        "id": "GuT-QnvyKoWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary modules for this project\n",
        "import pandas as pd\n",
        "from surprise import Dataset\n",
        "from surprise.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Ty8MyZxKKfIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the dataset from pandas, run this only once, you can comment out this part of the code after.\n",
        "!pip install pandas scikit-surprise"
      ],
      "metadata": {
        "id": "mGZBl3OnKUU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do the predictions work?\n",
        "\n",
        "1. **Model Training**:\n",
        "   - The SVD algorithm is first trained on a portion of the dataset, which includes user ratings for various movies.\n",
        "   - During training, the model learns to associate certain patterns and characteristics of users and movies with specific rating behaviors.\n",
        "\n",
        "2. **Latent Features Extraction**:\n",
        "   - SVD decomposes the rating matrix into matrices representing latent features of users and movies.\n",
        "   - These latent features capture underlying aspects that affect rating behavior but are not explicitly available in the data (like user preferences or movie characteristics).\n",
        "\n",
        "3. **Making Predictions**:\n",
        "   - Once the model is trained, it can predict ratings for user-movie pairs where the actual rating is unknown.\n",
        "   - The prediction is essentially a dot product of the latent features of the user and the movie. It represents the estimated preference of the user for that particular movie based on the learned patterns.\n",
        "\n",
        "4. **Example of a Prediction**:\n",
        "   - Suppose we want to predict how user `U` would rate movie `M`.\n",
        "   - The model uses the latent features it has learned for user `U` and movie `M` to compute a predicted rating.\n",
        "   - This prediction is a numerical value, typically on the same scale as the original ratings (e.g., 1 to 5).\n",
        "\n",
        "5. **Application**:\n",
        "   - These predictions are used to recommend movies to users.\n",
        "   - For example, the system can recommend movies that have the highest predicted ratings for a particular user.\n",
        "\n",
        "6. **Handling New Users or Movies (Cold Start Problem)**:\n",
        "   - One challenge is predicting ratings for new users or movies that have little to no rating history. This is known as the cold start problem.\n",
        "   - Solutions might involve using content-based approaches or hybrid models that don't rely solely on historical rating data."
      ],
      "metadata": {
        "id": "ukvxW2AgTjdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset.load_builtin('ml-100k')\n",
        "df = pd.DataFrame(data.raw_ratings, columns=[\"user\", \"item\", \"rating\", \"timestamp\"])\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "QNnS2A8HKcgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see the following columns:\n",
        "\n",
        "* **User ID**: A unique identifier for the user who provided the rating.\n",
        "\n",
        "* **Item ID (Movie ID)**: A unique identifier for the movie that was rated.\n",
        "\n",
        "* **Rating:** The rating given to the movie by the user. In the MovieLens 100k dataset, these ratings are typically on a scale of 1 to 5.\n",
        "\n",
        "* **Timestamp:** The time at which the rating was provided. The timestamp is usually in Unix time format, which counts seconds since the Unix epoch (January 1, 1970).\n",
        "\n"
      ],
      "metadata": {
        "id": "eU2Tw5HgNLVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - Describe the statistics of this dataset.\n",
        "# Hint: Use the describe() function\n"
      ],
      "metadata": {
        "id": "9_ntUWFALcZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will do some data preprocessing.\n",
        "\n",
        "This will include:\n",
        "*   Checking for missing values\n",
        "*   Converting timestamps to a readable format\n",
        "*   Splitting the data into testing and training subsets\n",
        "\n"
      ],
      "metadata": {
        "id": "sJoxe-JwMCKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "RZ_yvaO3LdgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there are no missing values."
      ],
      "metadata": {
        "id": "dgtjvngqMb0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert timestamp to a readable format\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "-k2Jp5r3MgGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into a training set and a test set\n",
        "trainset, testset = train_test_split(data, test_size=0.20)\n",
        "\n",
        "# Display the number of users and items in the training set\n",
        "print(f\"Number of users: {trainset.n_users}\")\n",
        "print(f\"Number of items: {trainset.n_items}\")\n",
        "\n",
        "# Display the first few elements of the test set\n",
        "print(testset[:5])"
      ],
      "metadata": {
        "id": "LF7oNwlFMnZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning in SVD\n",
        "Hyperparameter tuning is a critical step in optimizing the performance of an SVD model. The goal is to find the best combination of parameters that results in the most accurate predictions or lowest error rates.\n",
        "\n",
        "#### Hyperparameters we will be tuning in this project\n",
        "\n",
        "1. **`n_factors`**:\n",
        "   - Represents the number of latent factors (or features) to extract from the dataset.\n",
        "   - The values `[50, 100, 150]` are chosen to test the model's performance with a varying number of factors. A higher number of factors can capture more complex patterns but may lead to overfitting and increased computation time.\n",
        "\n",
        "2. **`n_epochs`**:\n",
        "   - Refers to the number of iterations over the entire dataset during training.\n",
        "   - The values `[20, 30]` provide a range to evaluate whether more iterations improve model performance or lead to overtraining.\n",
        "\n",
        "3. **`lr_all`** (Learning Rate):\n",
        "   - Determines the step size at each iteration while moving toward a minimum of the loss function.\n",
        "   - The values `[0.005, 0.010]` are chosen to test how fast the model learns. A smaller learning rate may lead to more precise convergence but requires more epochs.\n",
        "\n",
        "4. **`reg_all`** (Regularization Term):\n",
        "   - Helps prevent overfitting by penalizing larger model parameters.\n",
        "   - The values `[0.02, 0.1]` offer a range to assess the impact of regularization on model performance. Higher regularization can reduce overfitting but may lead to underfitting."
      ],
      "metadata": {
        "id": "Wl1g19D-MfmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a grid of SVD hyperparameters explained above for tuning\n",
        "param_grid = {\n",
        "    'n_factors': [50, 100, 150],\n",
        "    'n_epochs': [20, 30],\n",
        "    'lr_all': [0.005, 0.010],\n",
        "    'reg_all': [0.02, 0.1]\n",
        "}\n"
      ],
      "metadata": {
        "id": "Yt6AeZAzOmrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will train the model with the following parameters:\n",
        "\n",
        "1. **`SVD`**:\n",
        "   - This is the recommendation algorithm being tuned. SVD is a popular algorithm used in recommendation systems, particularly for matrix factorization.\n",
        "\n",
        "2. **`param_grid`**:\n",
        "   - This is a dictionary where keys are hyperparameter names, and values are lists of parameter settings to try as values. It defines the grid of parameters that will be tested.\n",
        "   - Example: If `param_grid` is `{'n_factors': [50, 100], 'lr_all': [0.005, 0.01]}`, GridSearchCV will evaluate the SVD algorithm for all combinations of `n_factors` and `lr_all` from these lists.\n",
        "\n",
        "3. **`measures=['RMSE', 'MAE']`**:\n",
        "   - These are the performance metrics used to evaluate the algorithm.\n",
        "   - `RMSE` stands for Root Mean Square Error, and `MAE` stands for Mean Absolute Error. Both are common metrics for evaluating the accuracy of prediction algorithms, with lower values indicating better performance.\n",
        "\n",
        "4. **`cv=3`**:\n",
        "   - This specifies the number of folds for cross-validation.\n",
        "   - In this context, `cv=3` means that a 3-fold cross-validation will be used. The dataset will be split into three parts: in each iteration, two parts will be used for training, and one part will be used for testing. This process repeats three times, each time with a different part used for testing."
      ],
      "metadata": {
        "id": "XbPGTyysRG5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise.model_selection import cross_validate, train_test_split, GridSearchCV\n",
        "from surprise import SVD, Dataset, Reader, accuracy\n",
        "\n",
        "# Perform grid search with cross-validation to find the best hyperparameters for our model\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['RMSE', 'MAE'], cv=3)\n",
        "gs.fit(data)"
      ],
      "metadata": {
        "id": "Ys3MuGk5Qph4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best score and parameters\n",
        "print(f\"Best RMSE: {gs.best_score['rmse']}\")\n",
        "print(f\"Best parameters: {gs.best_params['rmse']}\")"
      ],
      "metadata": {
        "id": "J85rTs0wQpYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Use the best model. Use best_estimator function on gs\n",
        "algo = ___________['rmse']"
      ],
      "metadata": {
        "id": "JVCuCpr0RRMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Train and test split. Make sure test_size is 0.25\n",
        "trainset, testset = ________\n",
        "# TODO - Fit the trainset to train the model\n",
        "algo.fit(_____)\n",
        "# TODO - Make predictions on the testset\n",
        "predictions = algo.test(_____)\n",
        "\n",
        "# TODO - Calculate and print RMSE on the predictions made\n",
        "accuracy.rmse(______)"
      ],
      "metadata": {
        "id": "N_rMgmt3SyGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict rating for a user and item\n",
        "user_id = '196'  # replace with a specific user ID\n",
        "item_id = '302'  # replace with a specific item (movie) ID\n",
        "predicted_rating = algo.predict(user_id, item_id)\n",
        "print(f\"Predicted rating for user {user_id} and item {item_id}: {predicted_rating.est}\")"
      ],
      "metadata": {
        "id": "Qn7yRR-aRXL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To inspect the predictions in detail, let's print the first 10 predictions made by the model\n",
        "for idx, prediction in enumerate(predictions[:10]):\n",
        "    print(f'Prediction {idx}: User {prediction.uid} and item {prediction.iid} has true rating {prediction.r_ui}, and the predicted rating is {prediction.est}')\n"
      ],
      "metadata": {
        "id": "TD1GVZ3FpxdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Rounding Numbers\n",
        "Rounding values is a technique used to simplify numbers, but its appropriateness depends on the context:\n",
        "\n",
        "**When to Round**\n",
        "1. **Simplification**: For estimations.\n",
        "2. **Reporting**: When exact figures aren't necessary (e.g., in everyday language).\n",
        "3. **Data Analysis**: To focus on significant trends by ignoring minor variations.\n",
        "4. **Financial Transactions**: Rounding to the smallest currency unit.\n",
        "5. **Display Purposes**: For clarity in graphs or tables.\n",
        "\n",
        "**When NOT to Round**\n",
        "1. **Intermediate Calculations**: Early rounding can lead to significant final errors.\n",
        "2. **Legal/Regulatory Documents**: Require exact figures.\n",
        "3. **Scientific/Engineering Work**: Precision is crucial.\n",
        "4. **Critical Calculations**: In health, safety, or finance, precision is essential.\n",
        "\n",
        "To summarize,\n",
        "- Rounding depends on the purpose and context of the calculation.\n",
        "- It is useful for simplification and clarity but should be avoided when precision is critical.\n",
        "- We must be aware of potential cumulative errors in sequential calculations.\n"
      ],
      "metadata": {
        "id": "idnQfHixB0HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us round the values of the predictions so that it falls within the rating categories of [1.0, 2.0, 3.0, 4.0, 5.0]"
      ],
      "metadata": {
        "id": "8TW56IrZLYej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - Round the prediction.est variable being printed. Use python's default rounding function to achieve this\n",
        "\n",
        "for idx, prediction in enumerate(predictions[:10]):\n",
        "    temp = math.ceil(int(prediction.est))\n",
        "    print(f'Prediction {idx}: User {prediction.uid} and item {prediction.iid} has true rating {prediction.r_ui}, and the predicted rating is {______(prediction.est)}')\n"
      ],
      "metadata": {
        "id": "qoKsEX-TLYEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT8Rpdt4ZPeA"
      },
      "source": [
        "<h3 align = 'center' >\n",
        "Thank you for completing the project!\n",
        "</h3>\n",
        "\n",
        "Please submit all materials to the NSDC HQ team at nsdc@nebigdatahub.org in order to receive a virtual certificate of completion. Do reach out to us if you have any questions or concerns. We are here to help you learn and grow.\n"
      ]
    }
  ]
}